# Concordância Inter e Intraobservador de Fraturas de Úmero Antes e Após a Exposição à Classificação AO com um Esquema Cognitivo 

<!-- https://mail.google.com/mail/u/0/?tab=wm#inbox/148310b6e0e3a869 -->

Anderson Reus Trevisol
Bárbara Mendes Boppré
Gustavo ou Renan
Gabriel El-Kouba
Ana Paula Bonilauri Ferreira
Ricardo Pietrobon

## Resumo

## Introdução

As fraturas de úmero são o terceiro tipo mais comum de fraturas em pessoas acima de 65 anos, depois de fraturas do rádio distal e fêmur proximal (court2006epidemiology). Os diversos sistemas de classificação de fraturas ortopédicas são propostos para melhorar a sua descrição, baseados na sua morfologia, no seu comportamento biológico e mecânico, e para orientar diretrizes terapêuticas. Apesar de termos evoluído nas últimas décadas na correlação da cognição e diagnóstico médico, ainda existem lacunas do conhecimento a serem respondidas em relação à essa intersecção no que diz respeito às classificações ortopédicas.

A classificação de Neer é amplamente utilizada na prática clínica e em pesquisa (@brorson2002improved), porém, a literatura sugere existir uma baixa concordância entre observadores utilizando este sistema de classificação (@siebenrock1993reproducibility; @sidor1993neer; @bernstein1996evaluation; @sjoden1997poor; @brorson2002improved; @shrader2005understanding; @brorson2009diagnosing). Este baixo índice de concordância pode ter associação ao uso inadequado do esquema cognitivo.
O conhecimento prévio construído em um “esquema” facilita muito o conhecimento contextual se a nova informação é assimilada no esquema (@ruiter2012achieve). Assim sendo, a interpretação das escalas deveria estar associada a um esquema cognitivo coerente (@ruiter2012achieve; @ferreira2010clinical), o que poderia contribuirpara o aumento da concordância entre observadores.

O objetivo deste estudo foi investigar a confiabilidade intra e interobservador da classificação de Neer para fraturas de úmero com um esquema cognitivo.



## Métodos

### Comitê de Ética

Este estudo foi aprovado pelo comitê de ética em pesquisa do Hospital Municipal São José (Joinville-SC) previamente ao início deste projeto (número do protocolo 239.511). Todos os participantes assinaram o termo de consentimento previamente à participação no estudo. 

### Participantes

Um total de 15 residentes de ortopedia participaram deste estudo, 5 eram do primeiro ano, 6 do segundo ano e 4 do terceiro ano. A média de idade era 28 anos e todos eram do gênero masculino.

### Image selection

Two second-year residents that were not enrolled directly with the study and an orthopedic surgeon, especialista in membro superior selected 20 images containing fractures with a wide pattern variation chosen to cover the full spectrum of the Neer classification for humerus fractures. As imagens eram na incidência ântero-posterior e perfil. As imagens foram obtidas de arquivos dos Hospital São Municipal São José (Joinville – SC). Qualquer sinal de identificação de pacientes foi removido. Imagens radiográficas mal posicionadas que poderiam gerar problemas na interpretação foram excluídas. Imagens de baixa qualidade ou com artefatos ou outros defeitos técnicos também foram excluídas.

### Neer Classification

O úmero proximal é dividido em 4 partes: cabeça, diáfise, tubérculo maior e tubérculo menor. É considerado desvio quando o segmento desloca-se mais de 1 cm (0,5 cm para o tubérculo maior) ou angulação maior do que 45º. As fraturas em uma parte são aquelas com pouco ou nenhum desvio, independentemente do número de traços de fratura; as fraturas em duas partes apresentam desvio de apenas uma parte (colo cirúrgico, colo anatômico, tubérculo maior ou tubérculo menor); as em três partes são fraturas da diáfise com um dos tubérculos, separados da cabeça; e as fraturas em quatro partes apresentam dissociação entre as quatro partes anatômicas do úmero proximal e alta incidência de necrose avascular (@neerii1970displaced). [http://radiopaedia.org/articles/proximal-humeral-fracture-classification-neer]

### Situated schemata extraction using Cognitive Task Analysis

For the purposes of our paper, we define a situated schema as the collection of concepts and situations (e.g., narratives) that an expert hand and upper extremity surgeon relates to each classification category. In order to extract the situated schema from our expert hand and upper extremity surgeon (GE), we used the following sequence. 

First, the Neer classification was presented to the hand and upper extremity surgery expert in an electronic format combining text and graphics for each classification category. Second, we asked the surgeon to "think aloud" about what they thought when finding a case in their daily practice. After an initial description, we specifically asked the expert to discuss any diagnostic, biomechanical or related therapeutic decision if it had not yet been previously mentioned. We also encouraged the expert to provide any narratives that might occur to him while thinking aloud about each classification category. The entire process was recorded in a video.

Second, the video was analyzed and a graph constituted by nodes and edges was built using [Graphviz](http://www.graphviz.org/). Each node represented either a concept or a situation, while edges connected relationships among diagnostic, biomechanical and therapeutic nodes.

### Procedimentos e Logística do Estudo

#### Avaliação Inicial

Na avaliação inicial, todos os residentes independentemente classificaram as 20 imagens de acordo com a Classificação de Neer. Todos os participantes simultaneamente, alocados na mesma sala, foram instruídos a não olhar as respostas dos outros ou discutir os casos clínicos. As imagens a serem classificadas estavam armazenadas na Plataforma online Edx [http://code.edx.org/]. As respostas eram armazenadas na própria plataforma online. Os residentes podiam consultar na internet o sistema de classificação de Neer. Não foi estipulado um tempo limite para classificar as imagens. Os autores do estudo não participaram como sujeitos do estudo.

#### Avaliação em 30 dias

Após 30 dias, cada residente independentemente classificou novamente as 20 imagens em uma ordem alterada na Plataforma Edx. Todos os outros procedimentos foram executados exatamente como descrito na sessão da Aavaliação Inicial.


#### Intervenção

The educational intervention was constituted by weekly sessions where participants completed 20 exercises related to the diagnosis, biomechanics and therapeutic planning of shoulder fractures. Os residentes, após terem respondido cada exercício, eram apresentados à resposta correta, juntamente com uma explicação justificando tal resposta. Os exercícios estavam organizados, na Plataforma online Edx, em “blocos” de 20 perguntas cada, ou seja, foram criados 4 blocos correspondentes as quatro semanas de intervenção. Os blocos de perguntas foram programados para serem liberados semanalmente, isto é, os residentes tinham acesso somente a um bloco de perguntas por semana. The full spectrum described by the Neer classification was covered based on cognitive schema from our expert hand and upper extremity surgeon (GE) as described in the previous session - Situated schemata extraction using Cognitive Task Analysis.


#### Pós-Intervenção, Avaliação em 60 dias

After the 4-week intervention period, all participants classified the same 20 images following the same protocol.


### Medidas avaliadas

Intra-observer agreement was measured by comparing ratings by the same participant between baseline and the thirty 30-day assessment. Baseline inter-observer agreement was measured at the 30-day assessment. The pre-post intervention evaluation was conducted by comparing the 30-day pre-intervention assessment with the 60-day post-intervention assessment.


### Análises dos dados

All data were extracted directly from [MySQL](http://www.mysql.com/) and [MongoDB](http://www.mongodb.org/) databases connected to the [Open edX](http://code.edx.org/) platforms. Data sets were then merged, also undergoing an exploratory graphical analysis to verify distributions, percentages, means and frequencies/percentages as well as rates of missing data. 
 
 

## Resultados

### Inter-observer reliability


### Intra-observer reliability


### Degree of improvement in inter-observer reliability after training


## Discussão


Até onde sabemos, esse é o primeiro estudo avaliando uma intervenção na tentativa de melhorar o grau de concordância entre observadores para a classificação de Neer de fraturas do ombro. Nossos resultados indicaram uma concordância que sem manteve relativamente estável durante o estudo, não tendo sido modificada em função da intervenção. Também encontramos que o grau de concordância é melhorado quando (1) a classificação se faz simplesmente por partes ao invés de feita com detalhes de cada fratura e (2) quando o numero de partes é diminuido. 

O baixo grau de concordância entre observadores nesse estudo concorda em grande parte com a literatura sobre classificações de fraturas ortopédicas <!-- ref -->. Essa baixa concordância se dá em grande parte pela complexidade das classificações, o que dá margem a interpretações diversas, especialmente por profissionais em treinamento e portanto com menos experiência. <!-- ref --> Apesar de que a intenção de se criar uma classificação que seja "completa" é louvável, a alta carga cognitiva exigida dos profissionais que a irão utilizar tende a fazer com que ela perca a sua praticidade. Esforços deveriam ser realizados portanto para a criação de escalas que sejam mais dinamicamente adaptadas a profissionais com diferentes graus de experiência na interpretação radiográfica. Por exemplo, profissionais que trabalhem em pronto socorros deveriam utilizar uma escala mais simplificada, enquanto sub-especialistas deveriam utilizar escalas mais detalhadas. O grau de detalhamento em cada uma destas subescalas seria definido através de estudos que identifiquem o grau de concordância obtida na prática clínica diária.

Apesar de que a nossa intervenção ter sido baseada em mecanismos bem estabelecidos na literatura sobre schemas cognitivos <!-- ref -->, não houve uma melhora da concordância como nós havíamos hipotetizado. Causas para a não melhora provavelmente se devem ao baixo tempo de exposição em relação à intervenção <!-- ref -->, e também ao fato de que esta exposição não ocorreu em um contexto clínico mas sim em um ambiente educacional artificial. Resultados superiores talvez pudessem ter sido encontrados se a intervenção educacional pudesse ter ocorrido durante a prática clínica diária, especificamente no momento em que os participante estivessem atendendo pacientes com fraturas de ombro. 

Apesar de o nosso artigo ser, até onde sabemos, o primeiro a conduzir uma intervenção na tentativa de melhorar o grau de concordância entre observadores, o nosso estudo tem limitações. Primeiro, a nossa intervenção utilizou um desenho pré-pós ao invés de um estudo randomizado. Na ausência de randomização, nós portanto não podemos fazer afirmações sobre relações causais entre a intervenção e a ausência de impacto sobre o grau de concordância. Estudos randomizados requerem no entanto amostras significativamente maiores, o que pode levar a barreiras logísticas em relação à sua execução. Segundo, esse estudo foi restrito a um grupo de participantes de uma única instituição, o que limita a sua generalizabilidade. Enquanto a participação de múltiplas instituições é sempre desejável, sociedades profissionais em ortopedia ainda não estão logisticamente organizadas para permitir estudos de maior escala, o que dificulta a sua realização. Por último, intervenções mais prolongadas e contextualizadas na prática diária teriam sido desejáveis, mas como nas limitações anteriores, a sua execução é limitada por fatores logísticos. 

Em conclusão, nós não recomendamos que intervenções educacionais curtas e descontextualizadas da prática clínica sejam utilizadas no aprendizado de classificações complexas. No entanto, a simplificação de tais classificações deveria ser considerada, levando a uma personalização da escala a ser utilizada a grupos de profissionais com graus de experiência diferentes. No que diz respeito a estudos futuros, recomendamos a utilização de estudos randomizados que permitam investigações causais, assim como a contextualização e personalização das intervenções educacionais. 



<!-- Ana, pros artigos abaixo precisava saber qual classificação --> <!-- Todos do Neer. -->

@bernstein1996evaluation


Substantial intraobserver reliability (K = 0.64) and moderate interobserver reproducibility (K = 0.52) were noted when the fractures in the present study were classified on the basis of radiographs alone. 


@brien1995neer

For the total group evaluated, percentage agreement between pairs of observers ranges from 57% to 71%, with the corresponding kappa values ranging from 0.37 to 0.75 (Table 2). Within the first subgroup of tuberosity
fractures (n = 18), the percentage agreement ranged from 44% to 83%, with corresponding kappa values from 0.24 to 0.77. In the second subgroup evaluated [neck fractures (n = 5)], the percentage agreement between pairs of
observers ranged from 60% to 100%, with kappa values of 0.41 to 1.00.
The results of the present study show only moderate agreement within observers for both the total number of patients studied and the tuberosity fracture group. In the group of neck fractures, however, the agreement
was as good as could be expected, with the neck fracture segment being the most easy to identify

@brorson2002low

Twenty-four doctors (nine orthopaedic residents, six fellows and nine specialists) participated in the study and classified all cases. Mean kappa for agreement between all pairs of observers was 0.27 (95% CI 0.26–0.28). No clinically important difference between the mean kappa values of residents, fellows and specialists was detected. Inexperienced doctors tended to classify more
fractures as displaced, but otherwise there were no differences between the three levels of experience.

@brorson2002improved

We found that two teaching sessions improved the overall mean kappa value from 0.27 (95% CI 0.23 to 0.31) to 0.62 (95% CI 0.57 to 0.67) for the Neer system. The improvement was particularly noticeable for the specialists in whom kappa increased from 0.30 (95% CI 0.23 to 0.37) to
0.79 (95% CI 0.70 to 0.88).

@brorson2012surgeons

At both classification rounds mean kappa-values for inter-observer agreement 
on Neer classification were 0.33 and 0.36.

@brunner2009impact

Interobserver agremment: According to group (I, II, III, IV) it showed moderate reliability with plain radiographs (κ = 0.48) and 2D CT scans (κ = 0.58), but improved to excellent after adding 3D volume rendering reconstructions (κ = 0.80).
Intraobserver reliability: For group (I, II, III, IV) it was moderate with plain radiographs (κ = 0.58), good with CT scans (κ = 0.69), and excellent with 3D volume rendering reconstructions (κ = 0.88).

@foroohar2011classification

Agreement of classification across all modalities was only “ slight”. For classification: X-ray > 3D CT
reconstruction > 2D CT scan with the kappa values being 0.14, 0.09, 0.07 respectively. Classification
among specialized upper extremity surgeons, overall all imaging modalities yielded low inter-observer
agreement.

@gumina2011comparison

Inter-observer reliability was K = 0.77 , while intra-observer reproducibility was K = 0.68 (examiner I) and K = 0.63 (examiner II).
After 6 years, the reproducibility of the neer classification was low for both examiners and not significantly dependent of the level of expertise





Até nosso atual conhecimento, este é o primeiro estudo que utilizou um esuqema cognitivo para avaliar o grau de concordância da classificação de Neer em fraturas de úmero. Nossos achados foram... 

http://goo.gl/NwMNN
http://goo.gl/17QP



### Discussion of the each of the main findings

Authors agreeing and discussion on mechanisms underlying the finding
Authors disagreeing with findings and reasons for disagreement


### Study limitations

Apesar de ser um estudo único na correlação entre a inserção de um esquema cognitivo e a concordância intra e interobservadores, nosso estuod paresenta limitações. Primeiro, nossa análise foi restrita a um grupo de residentes, e diferentes grupos poderiam apresentar uma variação na concordância. Segundo, na seleção das imagens para o estudo de concordância, nós escolhemos selecionar um amplo espectro de fraturas que muitas vezes não representam a prevalência atual destas fraturas na prática.


### Conclusions

First, provide readers with a description of future studies that should be conducted to further expand the field.
Point readers in relation to how the information presented in this manuscript might make a difference in practice.  In other words, how can the original information you have just generated be transformed into knowledge

                        
Some orthopedists have expressed concern, espe- cially in training programs, that more effort is spent trying to memorize classification systems for a num-ber of fractures, rather than truly understanding the fracture mechanics or the factors that have signifi- cant bearing on prognosis or treatment.  (The Journal of Hand Surgery 1996;21 A:574-582. Dennis J. Andersen,- ) do Olecranon Paper


## Referências
(autores, título, revista, ano, mês, número, volume, página)


---


# update.packages()

library(irr)
library(Agreement)
library(vcd)
require(lpSolve)
require(kappaSize)
require(boot)

# sample size

# http://www.ncbi.nlm.nih.gov/pubmed/22560852 - http://cran.r-project.org/web/packages/kappaSize/kappaSize.pdf
# https://etd.library.emory.edu/view/record/pid/emory:7t409



# descriptive

setwd("/Users/rpietro/articles/shoulder_agreement")

shoulder_inter_all  <- read.csv("shoulder_inter_all.csv")
shoulder_inter_all
agree(shoulder_inter_all)     # Simple percentage agreement

shoulder_inter_all_day1  <- read.csv("shoulder_inter_all_day1.csv")
shoulder_inter_all_day1
agree(shoulder_inter_all_day1)     # Simple percentage agreement

shoulder_inter_all_day30  <- read.csv("shoulder_inter_all_day30.csv")
shoulder_inter_all_day30
agree(shoulder_inter_all_day30)     # Simple percentage agreement

shoulder_inter_all_day60  <- read.csv("shoulder_inter_all_day60.csv")
shoulder_inter_all_day60
agree(shoulder_inter_all_day60)     # Simple percentage agreement


shoulder_inter_parts  <- read.csv("shoulder_inter_parts.csv")
shoulder_inter_parts
agree(shoulder_inter_parts)     # Simple percentage agreement
agree(shoulder_inter_parts,1)     # Simple percentage agreement


shoulder_inter_parts_day1  <- read.csv("shoulder_inter_parts_day1.csv")
shoulder_inter_parts_day1
agree(shoulder_inter_parts_day1)     # Simple percentage agreement
agree(shoulder_inter_parts_day1,1)     # Simple percentage agreement


shoulder_inter_parts_day30  <- read.csv("shoulder_inter_parts_day30.csv")
shoulder_inter_parts_day30
agree(shoulder_inter_parts_day30)     # Simple percentage agreement
agree(shoulder_inter_parts_day30,1)     # Simple percentage agreement


shoulder_inter_parts_day60  <- read.csv("shoulder_inter_parts_day60.csv")
shoulder_inter_parts_day60
agree(shoulder_inter_parts_day60)     # Simple percentage agreement
agree(shoulder_inter_parts_day60,1)     # Simple percentage agreement

# Intra-observer

shoulder_intra_parts_130  <- read.csv("shoulder_intra_parts_130.csv")
shoulder_intra_parts_130
agree(shoulder_intra_parts_130)     # Simple percentage agreement
agree(shoulder_intra_parts_130,1)     # Simple percentage agreement

# SexualFun
# (K <- Kappa(SexualFun))
# confint(K)
# agree <- agreementplot(SexualFun, main="Is sex fun?")
# We have thus produced an agreement plot, also called a Bangdiwala's Observer Agreement Chart. Note that our agreement plot is a representation of a k x k confusion matrix. The observed and expected diagonal elements are represented by superposed black and white rectangles, respectively. The extent to which the rectangles are above or below the line indicates the extent of any disagreement. (above and/or below indicates direction of the disagreement). The function also computes two statistic measuring the strength of agreement (relation of respective area sums). The first statistic is accessed using the term Bandiwala. This statistic is the unweighted agreement strength statistic. The second statistic makes an adjustment for ordered ratings, and is accessed using the code Bangdiwala_Weighted. Both statistics are measured on a scale from 0 to 1, where 1 indicates perfect agreement and 0 indicates perfect disagreement.
# unlist(agree)




# Interobserver

# data(diagnoses)
# head(diagnoses)
# df <- diagnoses[,1:3]
# head(df)
kappam.fleiss(shoulder_inter_all)
kappam.fleiss(shoulder_inter_all_day1)
kappam.fleiss(shoulder_inter_all_day30)
kappam.fleiss(shoulder_inter_all_day60)
kappam.fleiss(shoulder_inter_parts)
kappam.fleiss(shoulder_inter_parts_day1)
kappam.fleiss(shoulder_inter_parts_day30)
kappam.fleiss(shoulder_inter_parts_day60)
kappam.fleiss(shoulder_intra_parts_130)


# The unified approach calculates the agreement statistics for both continuous and categorical data to cover multiple readings from each of the n subjects.
# data(DCLHb);
# head(DCLHb)
# ua <- unified.agreement(dataset=DCLHb, var=c("HEMOCUE1","HEMOCUE2","SIGMA1","SIGMA2"), k=2, m=2, CCC_a_intra=0.9943, CCC_a_inter=0.9775, CCC_a_total=0.9775, CP_a=0.9, tran=1, TDI_a_intra=75, TDI_a_inter=150, TDI_a_total=150, error="const", dec=1, alpha=0.05);
# summary(ua);


#to obtain a 95%confidence interval of the four classification systems, using the boot package
ckappa.boot <- function(data,x) {ckappa(data[x,])[[2]]}
icsp <- boot(olecranon_speccoltvsnonspeccolt,ckappa.boot,1000)
quantile(icsp$t,c(0.025,0.975)) # two-sided bootstrapped confidence interval of kappa
boot.ci(icsp,type="bca") # adjusted bootstrap percentile (BCa) confidence interval (better)
icnsp <- boot(olecranon_specschatvsnonspecschat,ckappa.boot,1000)
quantile(icnsp$t,c(0.025,0.975)) # two-sided bootstrapped confidence interval of kappa
boot.ci(icnsp,type="bca") # adjusted bootstrap percentile (BCa) confidence interval (better)
icsp <- boot(olecranon_specmayvsnonspecmay,ckappa.boot,1000)
quantile(icsp$t,c(0.025,0.975)) # two-sided bootstrapped confidence interval of kappa
boot.ci(icsp,type="bca") # adjusted bootstrap percentile (BCa) confidence interval (better)
icnsp <- boot(olecranon_specaovsnonspecao,ckappa.boot,1000)
quantile(icnsp$t,c(0.025,0.975)) # two-sided bootstrapped confidence interval of kappa
boot.ci(icnsp,type="bca") # adjusted bootstrap percentile (BCa) confidence interval (better)

