# Concordância Inter e Intraobservador de Fraturas de Úmero Antes e Após a Exposição à Classificação AO com um Esquema Cognitivo 

<!-- https://mail.google.com/mail/u/0/?tab=wm#inbox/148310b6e0e3a869 -->

Anderson Reus Trevisol
Bárbara Mendes Boppré
Gustavo ou Renan
Gabriel El-Kouba
Ana Paula Bonilauri Ferreira
Ricardo Pietrobon

## Resumo

## Introdução

As fraturas de úmero são o terceiro tipo mais comum de fraturas em pessoas acima de 65 anos, depois de fraturas do rádio distal e fêmur proximal (court2006epidemiology). Os diversos sistemas de classificação de fraturas ortopédicas são propostos para melhorar a sua descrição, baseados na sua morfologia, no seu comportamento biológico e mecânico, e para orientar diretrizes terapêuticas. Apesar de termos evoluído nas últimas décadas na correlação da cognição e diagnóstico médico, ainda existem lacunas do conhecimento a serem respondidas em relação à essa intersecção no que diz respeito às classificações ortopédicas.

A classificação de Neer é amplamente utilizada na prática clínica e em pesquisa (@brorson2002improved), porém, a literatura sugere existir uma baixa concordância entre observadores utilizando este sistema de classificação (@siebenrock1993reproducibility; @sidor1993neer; @bernstein1996evaluation; @sjoden1997poor; @brorson2002improved; @shrader2005understanding; @brorson2009diagnosing). Este baixo índice de concordância pode ter associação ao uso inadequado do esquema cognitivo.
O conhecimento prévio construído em um “esquema” facilita muito o conhecimento contextual se a nova informação é assimilada no esquema (@ruiter2012achieve). Assim sendo, a interpretação das escalas deveria estar associada a um esquema cognitivo coerente (@ruiter2012achieve; @ferreira2010clinical), o que poderia contribuirpara o aumento da concordância entre observadores.

O objetivo deste estudo foi investigar a confiabilidade intra e interobservador da classificação de Neer para fraturas de úmero com um esquema cognitivo.



## Métodos

### Comitê de Ética

Este estudo foi aprovado pelo comitê de ética em pesquisa do Hospital Municipal São José (Joinville-SC) previamente ao início deste projeto (número do protocolo 239.511). Todos os participantes assinaram o termo de consentimento previamente à participação no estudo. 

### Participantes

Um total de 15 residentes de ortopedia participaram deste estudo, 5 eram do primeiro ano, 6 do segundo ano e 4 do terceiro ano. A média de idade era 28 anos e todos eram do gênero masculino.

### Image selection

Two second-year residents that were not enrolled directly with the study and an orthopedic surgeon, especialista in membro superior selected 20 images containing fractures with a wide pattern variation chosen to cover the full spectrum of the Neer classification for humerus fractures. As imagens eram na incidência ântero-posterior e perfil. As imagens foram obtidas de arquivos dos Hospital São Municipal São José (Joinville – SC). Qualquer sinal de identificação de pacientes foi removido. Imagens radiográficas mal posicionadas que poderiam gerar problemas na interpretação foram excluídas. Imagens de baixa qualidade ou com artefatos ou outros defeitos técnicos também foram excluídas.

### Neer Classification

O úmero proximal é dividido em 4 partes: cabeça, diáfise, tubérculo maior e tubérculo menor. É considerado desvio quando o segmento desloca-se mais de 1 cm (0,5 cm para o tubérculo maior) ou angulação maior do que 45º. As fraturas em uma parte são aquelas com pouco ou nenhum desvio, independentemente do número de traços de fratura; as fraturas em duas partes apresentam desvio de apenas uma parte (colo cirúrgico, colo anatômico, tubérculo maior ou tubérculo menor); as em três partes são fraturas da diáfise com um dos tubérculos, separados da cabeça; e as fraturas em quatro partes apresentam dissociação entre as quatro partes anatômicas do úmero proximal e alta incidência de necrose avascular (@neerii1970displaced). [http://radiopaedia.org/articles/proximal-humeral-fracture-classification-neer]

### Situated schemata extraction using Cognitive Task Analysis

For the purposes of our paper, we define a situated schema as the collection of concepts and situations (e.g., narratives) that an expert hand and upper extremity surgeon relates to each classification category. In order to extract the situated schema from our expert hand and upper extremity surgeon (GE), we used the following sequence. 

First, the Neer classification was presented to the hand and upper extremity surgery expert in an electronic format combining text and graphics for each classification category. Second, we asked the surgeon to "think aloud" about what they thought when finding a case in their daily practice. After an initial description, we specifically asked the expert to discuss any diagnostic, biomechanical or related therapeutic decision if it had not yet been previously mentioned. We also encouraged the expert to provide any narratives that might occur to him while thinking aloud about each classification category. The entire process was recorded in a video.

Second, the video was analyzed and a graph constituted by nodes and edges was built using [Graphviz](http://www.graphviz.org/). Each node represented either a concept or a situation, while edges connected relationships among diagnostic, biomechanical and therapeutic nodes.

### Procedimentos e Logística do Estudo

#### Avaliação Inicial

Na avaliação inicial, todos os residentes independentemente classificaram as 20 imagens de acordo com a Classificação de Neer. Todos os participantes simultaneamente, alocados na mesma sala, foram instruídos a não olhar as respostas dos outros ou discutir os casos clínicos. As imagens a serem classificadas estavam armazenadas na Plataforma online Edx [http://code.edx.org/]. As respostas eram armazenadas na própria plataforma online. Os residentes podiam consultar na internet o sistema de classificação de Neer. Não foi estipulado um tempo limite para classificar as imagens. Os autores do estudo não participaram como sujeitos do estudo.

#### Avaliação em 30 dias

Após 30 dias, cada residente independentemente classificou novamente as 20 imagens em uma ordem alterada na Plataforma Edx. Todos os outros procedimentos foram executados exatamente como descrito na sessão da Aavaliação Inicial.


#### Intervenção

The educational intervention was constituted by weekly sessions where participants completed 20 exercises related to the diagnosis, biomechanics and therapeutic planning of shoulder fractures. Os residentes, após terem respondido cada exercício, eram apresentados à resposta correta, juntamente com uma explicação justificando tal resposta. Os exercícios estavam organizados, na Plataforma online Edx, em “blocos” de 20 perguntas cada, ou seja, foram criados 4 blocos correspondentes as quatro semanas de intervenção. Os blocos de perguntas foram programados para serem liberados semanalmente, isto é, os residentes tinham acesso somente a um bloco de perguntas por semana. The full spectrum described by the Neer classification was covered based on cognitive schema from our expert hand and upper extremity surgeon (GE) as described in the previous session - Situated schemata extraction using Cognitive Task Analysis.


#### Pós-Intervenção, Avaliação em 60 dias

After the 4-week intervention period, all participants classified the same 20 images following the same protocol.


### Medidas avaliadas

Intra-observer agreement was measured by comparing ratings by the same participant between baseline and the thirty 30-day assessment. Baseline inter-observer agreement was measured at the 30-day assessment. The pre-post intervention evaluation was conducted by comparing the 30-day pre-intervention assessment with the 60-day post-intervention assessment.


### Análises dos dados

All data were extracted directly from [MySQL](http://www.mysql.com/) and [MongoDB](http://www.mongodb.org/) databases connected to the [Open edX](http://code.edx.org/) platforms. Data sets were then merged, also undergoing an exploratory graphical analysis to verify distributions, percentages, means and frequencies/percentages as well as rates of missing data. 
 
Apenas observadores que haviam completado um determinado grupo de observações (dia 1, dia 30 ou dia 60) foram considerados na análise. Porcentagens de concordância assim como valores de Kappa Fleiss foram reportados. Kappa Fleiss é uma medida de concordância para variáveis categóricas que leva em consideração a possível concordância ao acaso. @fleiss1973equivalence  Por fim, a comparação entre os valores de Kappa pré e pós intervenção (dias 30 e 60, respectivamente) foram estimadas através da computação de erros padrão e intervalos de confiança (95%) utilizando bootstrap. <!-- check with joao for reference -->
 

## Resultados

### Resultados descritivos

Quando todos os 9 observadores completando as 3 avaliações foram considerados, não houve nenhuma instância em que houvesse concordância completa entre todos. O mesmo ocorreu quando a avaliação se restringiu ao dias isolados 1 (15 observadores), 30 (11 observadores) e 60 (13 observadores). No entanto, a porcentagem de concordância entre os observadores foi aumentada quando a classificação foi simplificada para incorporar apenas o número de partes e com 1 grau de tolerância: 21.4% para o dia 1 (15 observadores), 40% para o dia 30 (11 observadores) e 45% para o dia 60 (13 observadores). 

Em relação a porcentagem de concordância com o mesmo indivíduo entre os dias 1 e 30, a taxa foi de 28.5%, chegando a 65.7% quando um grau de tolerância foi adicionado. 


### Valores de kappa

Os valores de Kappa Fleiss foram de 0.302 (p < 0.001) para o estudo como um todo (9 observadores), 0.256 (p < 0.001) para o dia 1, 0.277  (p < 0.001) para o dia 30, 0.315  (p < 0.001) para o dia 60. A diferença entre os dias 30 (pré-intervenção) e 60 (pós-intervenção) não foi estatisticamente significativa. 

Quando a avaliação se restringiu ao número de partes, os valores de Kappa Fleiss chegaram a 0.279  (p < 0.001) para o grupo como um todo, 0.26  (p < 0.001) para o dia 30 e 0.332 para o dia 60. A diferença entre os dias 30 (pré-intervenção) e 60 (pós-intervenção) não foi estatisticamente significativa. A concordância entre os mesmos indivíduos, comparando os dias 1 e 30, foi de 0.01 (p = 0.809)


## Discussão


Até onde sabemos, esse é o primeiro estudo avaliando uma intervenção na tentativa de melhorar o grau de concordância entre observadores para a classificação de Neer de fraturas do ombro. Nossos resultados indicaram uma concordância que se manteve relativamente estável durante o estudo, não tendo sido modificada em função da intervenção. Também encontramos que o grau de concordância é melhorado quando (1) a classificação se faz simplesmente por partes ao invés de feita com detalhes de cada fratura e (2) quando o número de partes é diminuido.

O baixo grau de concordância entre observadores nesse estudo corrobora com artigos anteriores sobre classificações de fraturas ortopédicas (@siebenrock1993reproducibility; @sidor1993neer; @bernstein1996evaluation; @sjoden1997poor; @brorson2002improved; @shrader2005understanding; @brorson2009diagnosing). A complexidade das classificações pode conduzir à essa baixa concordância, levando à diversas interpretações, principalmente por médicos em treinamento e com menos experiência (@kristiansen1988neer; @foroohar2011classification). Apesar de que, a intenção de se criar uma classificação que seja "completa" é louvável, a sua praticidade pode ser disperdiçada em função da alta carga cognitiva exigida dos profissionais que a irão utilizar. A criação de escalas mais condizentes com diferentes níveis de experiência na interpretação radiográfica poderiam ser melhor aproveitadas.Ou seja, dependendo da área de atuação do profissional, escalas mais simplificadas ou mais complexas poderiam ser utilizadas. Por exemplo, médicos sub-especialistas utilizariam escalas mais detalhadas, enquanto médicos que trabalhem em pronto socorros utilizariam escalas mais simplificadas. O grau de detalhamento em cada uma destas escalas distintas seria determinado através de estudos que identifiquem o grau de concordância obtido na prática clínica diária.

Apesar do nosso treinamento ter sido baseado em esquemas cognitivos estabelecidos na literatura (@regehr1996issues; @ruiter2012achieve), não houve uma melhora da concordância como nós esperávamos. Esquemas cognitivos situados partem do princípio de que o cérebro raciocina não apenas através de informações armazenadas nele, mas utilizando também fatores ambientais como tecnologias, contatos sociais, experiências prévias, entre outros fatores (@van2010cognitive). O fato do tempo do treinamento não ter sido muito longo pode ter relação com o não aumento da concordância entre observadores <!-- ref -->, assim como a exposição não ter acontecido em um contexto clínico, e sim, em um ambiente educacional artificial. Se a intervenção educacional tivesse ocorrido durante a prática clínica diária, ou seja, no momento de atendimento a pacientes com fraturas de ombro, talvez os resultados poderiam ter sido superiores.

Nosso estudo apresenta limitações, mesmo até onde sabemos, ser o primeiro a inserir uma intervenção, baseada em esquemas cognitivos, na tentativa de melhorar o grau de concordância entre observadores. Primeiro, a nossa intervenção educacional utilizou um desenho pré-pós ao invés de um estudo randomizado. Na ausência de randomização, não se pode fazer afirmações sobre relações causais entre a intervenção e a ausência de impacto sobre o grau de concordância. No entanto, amostras significantemente maiores são requeridas em estudos randomizados, o que pode dificultar sua execução devido à barreiras logísticas. Segundo, o fato deste estudo ter sido realizado em uma única instituição, não podemos generalizar os resultados. A participação de várias instituições é sempre desejável, porém há um envolvimento logístico para a realização de estudos dessa natureza, o que dificulta o processo. Por último, um tempo maior de treinamento, bem como a contextualização na prática clínica diária são aspectos ideais a serem inderidos no estudo. Porém, como nas limitações anteriores, fatores logísticos dificultam a sua realização.

Concluímos que intervenções educacionais curtas e descontextualizadas da prática clínica não são recomendadas no aprendizado de classificações complexas. No entanto, consideramos que uma personalização de escalas relacionada a graus de experiências diferentes poderia ser de maior aproveitamento. Em relação a estudos futuros, recomendaríamos estudos randomizados que permitam investigações causais, assim como a contextualização e personalização das intervenções educacionais. 



<!-- Ana, pros artigos abaixo precisava saber qual classificação --> <!-- Todos do Neer. -->

<!-- @bernstein1996evaluation


Substantial intraobserver reliability (K = 0.64) and moderate interobserver reproducibility (K = 0.52) were noted when the fractures in the present study were classified on the basis of radiographs alone. 


@brien1995neer

For the total group evaluated, percentage agreement between pairs of observers ranges from 57% to 71%, with the corresponding kappa values ranging from 0.37 to 0.75 (Table 2). Within the first subgroup of tuberosity
fractures (n = 18), the percentage agreement ranged from 44% to 83%, with corresponding kappa values from 0.24 to 0.77. In the second subgroup evaluated [neck fractures (n = 5)], the percentage agreement between pairs of
observers ranged from 60% to 100%, with kappa values of 0.41 to 1.00.
The results of the present study show only moderate agreement within observers for both the total number of patients studied and the tuberosity fracture group. In the group of neck fractures, however, the agreement
was as good as could be expected, with the neck fracture segment being the most easy to identify

@brorson2002low

Twenty-four doctors (nine orthopaedic residents, six fellows and nine specialists) participated in the study and classified all cases. Mean kappa for agreement between all pairs of observers was 0.27 (95% CI 0.26–0.28). No clinically important difference between the mean kappa values of residents, fellows and specialists was detected. Inexperienced doctors tended to classify more
fractures as displaced, but otherwise there were no differences between the three levels of experience.

@brorson2002improved

We found that two teaching sessions improved the overall mean kappa value from 0.27 (95% CI 0.23 to 0.31) to 0.62 (95% CI 0.57 to 0.67) for the Neer system. The improvement was particularly noticeable for the specialists in whom kappa increased from 0.30 (95% CI 0.23 to 0.37) to
0.79 (95% CI 0.70 to 0.88).

@brorson2012surgeons

At both classification rounds mean kappa-values for inter-observer agreement 
on Neer classification were 0.33 and 0.36.

@brunner2009impact

Interobserver agremment: According to group (I, II, III, IV) it showed moderate reliability with plain radiographs (κ = 0.48) and 2D CT scans (κ = 0.58), but improved to excellent after adding 3D volume rendering reconstructions (κ = 0.80).
Intraobserver reliability: For group (I, II, III, IV) it was moderate with plain radiographs (κ = 0.58), good with CT scans (κ = 0.69), and excellent with 3D volume rendering reconstructions (κ = 0.88).

@foroohar2011classification

Agreement of classification across all modalities was only “ slight”. For classification: X-ray > 3D CT
reconstruction > 2D CT scan with the kappa values being 0.14, 0.09, 0.07 respectively. Classification
among specialized upper extremity surgeons, overall all imaging modalities yielded low inter-observer
agreement.

@gumina2011comparison

Inter-observer reliability was K = 0.77 , while intra-observer reproducibility was K = 0.68 (examiner I) and K = 0.63 (examiner II).
After 6 years, the reproducibility of the neer classification was low for both examiners and not significantly dependent of the level of expertise





Até nosso atual conhecimento, este é o primeiro estudo que utilizou um esuqema cognitivo para avaliar o grau de concordância da classificação de Neer em fraturas de úmero. Nossos achados foram... 

http://goo.gl/NwMNN
http://goo.gl/17QP



### Discussion of the each of the main findings

Authors agreeing and discussion on mechanisms underlying the finding
Authors disagreeing with findings and reasons for disagreement


### Study limitations

Apesar de ser um estudo único na correlação entre a inserção de um esquema cognitivo e a concordância intra e interobservadores, nosso estuod paresenta limitações. Primeiro, nossa análise foi restrita a um grupo de residentes, e diferentes grupos poderiam apresentar uma variação na concordância. Segundo, na seleção das imagens para o estudo de concordância, nós escolhemos selecionar um amplo espectro de fraturas que muitas vezes não representam a prevalência atual destas fraturas na prática.


### Conclusions

First, provide readers with a description of future studies that should be conducted to further expand the field.
Point readers in relation to how the information presented in this manuscript might make a difference in practice.  In other words, how can the original information you have just generated be transformed into knowledge

                        
Some orthopedists have expressed concern, espe- cially in training programs, that more effort is spent trying to memorize classification systems for a num-ber of fractures, rather than truly understanding the fracture mechanics or the factors that have signifi- cant bearing on prognosis or treatment.  (The Journal of Hand Surgery 1996;21 A:574-582. Dennis J. Andersen,- ) do Olecranon Paper


## Referências
(autores, título, revista, ano, mês, número, volume, página)





# update.packages()

library(irr)
library(Agreement)
library(vcd)
require(lpSolve)
require(kappaSize)
require(boot)

# sample size

# http://www.ncbi.nlm.nih.gov/pubmed/22560852 - http://cran.r-project.org/web/packages/kappaSize/kappaSize.pdf
# https://etd.library.emory.edu/view/record/pid/emory:7t409



# descriptive

setwd("/Users/rpietro/articles/shoulder_agreement")

shoulder_inter_all  <- read.csv("shoulder_inter_all.csv")
# shoulder_inter_all
agree(shoulder_inter_all)     # Simple percentage agreement

shoulder_inter_all_day1  <- read.csv("shoulder_inter_all_day1.csv")
# shoulder_inter_all_day1
agree(shoulder_inter_all_day1)     # Simple percentage agreement

shoulder_inter_all_day30  <- read.csv("shoulder_inter_all_day30.csv")
# shoulder_inter_all_day30
agree(shoulder_inter_all_day30)     # Simple percentage agreement

shoulder_inter_all_day60  <- read.csv("shoulder_inter_all_day60.csv")
# shoulder_inter_all_day60
agree(shoulder_inter_all_day60)     # Simple percentage agreement


shoulder_inter_parts  <- read.csv("shoulder_inter_parts.csv")
# shoulder_inter_parts
agree(shoulder_inter_parts)     # Simple percentage agreement
agree(shoulder_inter_parts,1)     # Simple percentage agreement


shoulder_inter_parts_day1  <- read.csv("shoulder_inter_parts_day1.csv")
# shoulder_inter_parts_day1
agree(shoulder_inter_parts_day1)     # Simple percentage agreement
agree(shoulder_inter_parts_day1,1)     # Simple percentage agreement


shoulder_inter_parts_day30  <- read.csv("shoulder_inter_parts_day30.csv")
# shoulder_inter_parts_day30
agree(shoulder_inter_parts_day30)     # Simple percentage agreement
agree(shoulder_inter_parts_day30,1)     # Simple percentage agreement


shoulder_inter_parts_day60  <- read.csv("shoulder_inter_parts_day60.csv")
# shoulder_inter_parts_day60
agree(shoulder_inter_parts_day60)     # Simple percentage agreement
agree(shoulder_inter_parts_day60,1)     # Simple percentage agreement

# Intra-observer

shoulder_intra_parts_130  <- read.csv("shoulder_intra_parts_130.csv")
# shoulder_intra_parts_130
agree(shoulder_intra_parts_130)     # Simple percentage agreement
agree(shoulder_intra_parts_130,1)     # Simple percentage agreement

# SexualFun
# (K <- Kappa(SexualFun))
# confint(K)
# agree <- agreementplot(SexualFun, main="Is sex fun?")
# We have thus produced an agreement plot, also called a Bangdiwala's Observer Agreement Chart. Note that our agreement plot is a representation of a k x k confusion matrix. The observed and expected diagonal elements are represented by superposed black and white rectangles, respectively. The extent to which the rectangles are above or below the line indicates the extent of any disagreement. (above and/or below indicates direction of the disagreement). The function also computes two statistic measuring the strength of agreement (relation of respective area sums). The first statistic is accessed using the term Bandiwala. This statistic is the unweighted agreement strength statistic. The second statistic makes an adjustment for ordered ratings, and is accessed using the code Bangdiwala_Weighted. Both statistics are measured on a scale from 0 to 1, where 1 indicates perfect agreement and 0 indicates perfect disagreement.
# unlist(agree)




# Interobserver

# data(diagnoses)
# head(diagnoses)
# df <- diagnoses[,1:3]
# head(df)
kappam.fleiss(shoulder_inter_all)
kappam.fleiss(shoulder_inter_all_day1)
kappam.fleiss(shoulder_inter_all_day30)
kappam.fleiss(shoulder_inter_all_day60)
kappam.fleiss(shoulder_inter_parts)
kappam.fleiss(shoulder_inter_parts_day1)
kappam.fleiss(shoulder_inter_parts_day30)
kappam.fleiss(shoulder_inter_parts_day60)
kappam.fleiss(shoulder_intra_parts_130)


# The unified approach calculates the agreement statistics for both continuous and categorical data to cover multiple readings from each of the n subjects.
# data(DCLHb);
# head(DCLHb)
# ua <- unified.agreement(dataset=DCLHb, var=c("HEMOCUE1","HEMOCUE2","SIGMA1","SIGMA2"), k=2, m=2, CCC_a_intra=0.9943, CCC_a_inter=0.9775, CCC_a_total=0.9775, CP_a=0.9, tran=1, TDI_a_intra=75, TDI_a_inter=150, TDI_a_total=150, error="const", dec=1, alpha=0.05);
# summary(ua);


#to obtain a 95%confidence interval of the four classification systems, using the boot package
ckappa.boot <- function(data,x) {ckappa(data[x,])[[2]]}
icsp <- boot(olecranon_speccoltvsnonspeccolt,ckappa.boot,1000)
quantile(icsp$t,c(0.025,0.975)) # two-sided bootstrapped confidence interval of kappa
boot.ci(icsp,type="bca") # adjusted bootstrap percentile (BCa) confidence interval (better)
icnsp <- boot(olecranon_specschatvsnonspecschat,ckappa.boot,1000)
quantile(icnsp$t,c(0.025,0.975)) # two-sided bootstrapped confidence interval of kappa
boot.ci(icnsp,type="bca") # adjusted bootstrap percentile (BCa) confidence interval (better)
icsp <- boot(olecranon_specmayvsnonspecmay,ckappa.boot,1000)
quantile(icsp$t,c(0.025,0.975)) # two-sided bootstrapped confidence interval of kappa
boot.ci(icsp,type="bca") # adjusted bootstrap percentile (BCa) confidence interval (better)
icnsp <- boot(olecranon_specaovsnonspecao,ckappa.boot,1000)
quantile(icnsp$t,c(0.025,0.975)) # two-sided bootstrapped confidence interval of kappa
boot.ci(icnsp,type="bca") # adjusted bootstrap percentile (BCa) confidence interval (better)

 -->